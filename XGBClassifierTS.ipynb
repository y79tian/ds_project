{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import shap\n",
    "\n",
    "def SplitTrainTestValid (dataset, testRatio = 0.2, validRatio = 0.2): \n",
    "    \"\"\"Split the pandas dataframe in the training, validation and test datset with the desired ratios\n",
    "    \n",
    "    Args:\n",
    "        dataFrame (pd.DataFrame): name of the whole dataset we want to split\n",
    "        testRatio (float): ratio of the test data in the whole dataset\n",
    "        validRatio (float): the ratio of the validation data in the whole dataset\n",
    "    \n",
    "    Return:\n",
    "        pd.Dataframe: train_X for features in the training data\n",
    "        numpy array: train_y\n",
    "        pd.Dataframe: valid_X\n",
    "        numpy array: valid_y\n",
    "        pd.Dataframe: test_x\n",
    "        numpy array: test_y\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the size for each type of data\n",
    "    totalLen = dataset.shape[0];\n",
    "    trainLen = int(totalLen * (1 - testRatio - validRatio));\n",
    "    testLen = int(totalLen * testRatio);\n",
    "    \n",
    "    test = dataset.iloc[-testLen:, :]\n",
    "    train_valid = dataset.iloc[:-testLen, :]\n",
    "    \n",
    "    train_X, train_y = train_valid.iloc[:trainLen, :-1], train_valid.iloc[:trainLen, -1].values\n",
    "    valid_X, valid_y = train_valid.iloc[trainLen:, :-1], train_valid.iloc[trainLen:, -1].values\n",
    "    test_X, test_y = test.iloc[:, :-1], test.iloc[:, -1].values\n",
    "    \n",
    "    return train_X, train_y, valid_X, valid_y, test_X, test_y\n",
    "\n",
    "\n",
    "\n",
    "def XGBClassiferForSurvey (fileName, testRatio=0.2, validRatio = 0.2, maxDepth=9, learningRate=0.15, subsample=0.9, \n",
    "                           colsample_bytree=0.3, objective='binary:logistic', numIters=100, minChildWeight = 1, gamma = 0):\n",
    "    \"\"\"predict whether the surveyee will be craving before the next survey using XGBClassifier\n",
    "    \n",
    "    Args: \n",
    "        fileName (str): name of file to read in data from \n",
    "        testRatio (float) : ratio of the test data in the whole dataset\n",
    "        validRatio (float) : the ratio of the validation data in the whole dataset\n",
    "        maxDepth (int): maximum depth of tree  (optional: default = 9)\n",
    "        learningRate (float): learning rate of XGB model, range [0, 1]  (optional: default = 0.15)\n",
    "        subsample (float): fraction of observations to be randomly sampled for each tree (optional: default = 0.9)\n",
    "        colsample_bytree (float): fraction of columns to be randomly sampled for each tree (optional: default = 0.3)\n",
    "        objective (str): loss function  (optional: default='binary:logistic')\n",
    "        numIters (int): number of iterations for XGB model  (optional: default = 100)\n",
    "        minChildWeight (float): controls when the tree building should terminate based comparing the sum of the \n",
    "            instance weights to the minChildWeight.\n",
    "        gamma (float): Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "    Return:\n",
    "        bool: whether craving for the next time of survey\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # read in the raw data \n",
    "    # we will not sample data here because it is unlikely that we will have over 100k of data\n",
    "    df = pd.read_parquet(fileName, engine='auto')\n",
    "    \n",
    "    # work with NaN values, then update df\n",
    "    # we will figure out how to deal with NaN after EDA \n",
    "    df = df\n",
    "    \n",
    "    # transform the y variable to 0 and 1(xgb cannot handle factors well)\n",
    "    df['is_craving'] = df['is_craving'].apply(lambda x: 0 if x=='False' else 1)\n",
    "    \n",
    "    # lead the y variable (it becomes the value we want to predict -- label)\n",
    "    df['craving_lead'] = df['is_craving'].shift(-1)\n",
    "    df[\"craving_lead\"].fillna(0, inplace = True)\n",
    "    \n",
    "    # maybe add some lags for other features \n",
    "    \n",
    "    # move craving_lead to the last column (skip if label is aleady the last colum)\n",
    "    new_cols = [col for col in df.columns if col != 'craving_lead'] + ['craving_lead']\n",
    "    df = df[new_cols]\n",
    "    feature_columns = new_cols[:-1]\n",
    "    \n",
    "    # Split data\n",
    "    train_X, train_y, valid_X, valid_y, test_X, test_y = SplitTrainTestValid (df, testRatio, validRatio)\n",
    "    \n",
    "    # Create XGBoost matrices\n",
    "    Train = xgb.DMatrix(train_X, label = train_y)\n",
    "    Valid = xgb.DMatrix(valid_X, label = valid_y)\n",
    "    Test = xgb.DMatrix(test_X, label = test_y)\n",
    "    \n",
    "    # Setting XGBoost parameters\n",
    "    parameter = {'learning_rate': learningRate,\n",
    "                 'max_depth': maxDepth,\n",
    "                 'colsample_bytree': colsample_bytree,\n",
    "                 'min_child_weight': minChildWeight,\n",
    "                 'subsample': subsample,\n",
    "                 'gamma': gamma,\n",
    "                 'eval_metrix': \"auc\",\n",
    "                 'objective': objective\n",
    "    }\n",
    "    \n",
    "    # Fine tune the above hyperparameters towards the validation dataset, and get the optimal params\n",
    "    optimalParams = parameters\n",
    "    \n",
    "    # Run XGBoost\n",
    "    model = xgb.train(params = optimalParams, train = Train,num_boost_round = numIters, \n",
    "                      evals = [(Test, \"Yes\")], verbose_eval = 50)\n",
    "    # Predictions\n",
    "    predictions = model.predict(Test)\n",
    "    predictions = np.where(predictions > 0.5, 1, 0)\n",
    "    \n",
    "   # Confusion Matrix\n",
    "    cm = confusion_matrix(test_y, predictions)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion Matrix')\n",
    "    fig.colorbar(cax)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print report\n",
    "    report = classification_report(test_y, predictions)\n",
    "    print(report)\n",
    "    \n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(test_y, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([-0.02, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # feature importances\n",
    "    xgb.plot_importance(model, max_num_features = 10)\n",
    "    \n",
    "    # Prepare and plot SHAP \n",
    "    # monkey patch\n",
    "    model_bytearray = finalModel.save_raw()[4:]\n",
    "    def myfun(self=None):\n",
    "        return model_bytearray\n",
    "    finalModel.save_raw = myfun\n",
    "    # plot SHAP\n",
    "    explainer = shap.TreeExplainer(finalModel)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values,\n",
    "                      X_test,\n",
    "                      feature_names = feature_columns,\n",
    "                      max_display = 10)\n",
    "    \n",
    "    return predictions[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
