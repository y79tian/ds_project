{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def XGBClassiferForSurvey (fileName, testRatio = 0.2):\n",
    "    \"\"\"predict whether the surveyee will be craving given the survey\n",
    "    \n",
    "    Args: \n",
    "        fileName (str): name of file to read in data from \n",
    "        testRatio (float) : ratio of the test data in the whole dataset\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    # Read in the raw data \n",
    "    # We will not sample data here because it is unlikely that we will have over 100k of data\n",
    "    df = pd.read_parquet(fileName, engine='auto')\n",
    "    \n",
    "    # Work with NaN values, then update df\n",
    "    # We will figure out how to deal with NaN after EDA \n",
    "    df = df\n",
    "    \n",
    "    # Transform the y variable to 0 and 1(xgb cannot handle factors well)\n",
    "    df['is_craving'] = df['is_craving'].apply(lambda x: 0 if x=='False' else 1)\n",
    "    \n",
    "    # Move is_craving to the last column (skip if label is aleady the last colum)\n",
    "    new_cols = [col for col in df.columns if col != 'is_craving'] + ['is_craving']\n",
    "    df = df[new_cols]\n",
    "    feature_columns = new_cols[:-1]\n",
    "\n",
    "    # Isolate the x and y variables\n",
    "    y = df.iloc[:, -1].values\n",
    "    X = df.iloc[:, :-1].values\n",
    "    \n",
    "    #split dataset into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testRatio)\n",
    "    \n",
    "    # Create xgboost matrices\n",
    "    Train = xgb.DMatrix(X_train, label = y_train, feature_names = feature_columns)\n",
    "    Test = xgb.DMatrix(X_test, label = y_test, feature_names = feature_columns)\n",
    "    \n",
    "    # Tuning parameters: Round 1\n",
    "    print(\"=====Parameter Tunning Round 1=====\")\n",
    "    \n",
    "    tune_control = KFold(n_splits = 5, shuffle = True).split(X = X_train, y = y_train)\n",
    "    tune_grid = {'learning_rate': [0.05, 0.3],\n",
    "                 'max_depth': range(2, 9, 2),\n",
    "                 'colsample_bytree': [0.5, 1],\n",
    "                 'subsample': [1],\n",
    "                 'min_child_weight': [1],\n",
    "                 'gamma': [0], \n",
    "                 'random_state': [1502],\n",
    "                 'n_estimators': range(200, 2000, 200),\n",
    "                 'booster': [\"gbtree\"]}\n",
    "    \n",
    "    classifier = XGBClassifier(objective = \"binary:logistic\")\n",
    "    \n",
    "    # Cross Validation Assembly\n",
    "    nJobs = multiprocessing.cpu_count()\n",
    "    grid_search = GridSearchCV(estimator = classifier,\n",
    "                               param_grid = tune_grid,\n",
    "                               scoring = \"roc_auc\",\n",
    "                               n_jobs = nJobs,\n",
    "                               cv = tune_control,\n",
    "                               verbose = 5)\n",
    "\n",
    "    # Setting evaluation parameters\n",
    "    evaluation_parameters = {\"early_stopping_rounds\": 100,\n",
    "                             \"eval_metric\": \"auc\",\n",
    "                             \"eval_set\": [(X_test, y_test)]}\n",
    "\n",
    "    # Hyperparameter tuning and cross validation\n",
    "    tune_model = grid_search.fit(X = X_train,\n",
    "                                 y = y_train,\n",
    "                                 **evaluation_parameters)\n",
    "    grid_search.best_params_, grid_search.best_score_\n",
    "    \n",
    "    \n",
    "    # Tuning parameters: Round 2\n",
    "    print(\"=====Parameter Tunning Round 2=====\")\n",
    "    \n",
    "    tune_control = KFold(n_splits = 5, shuffle = True).split(X = X_train, y = y_train)\n",
    "    tune_grid2 = {'learning_rate': [grid_search.best_params_.get('learning_rate')],\n",
    "                   'max_depth': [grid_search.best_params_.get('max_depth')],\n",
    "                   'colsample_bytree': [grid_search.best_params_.get('colsample_bytree')],\n",
    "                   'subsample': [0.9, 1],\n",
    "                   'min_child_weight': range(1,5,1),\n",
    "                   'gamma': [0, 0.1], \n",
    "                   'n_estimators': range(200, 2000, 200),\n",
    "                   'booster': [\"gbtree\"]}                                                \n",
    "\n",
    "    #Cross Validation Assembly\n",
    "    grid_search2 = GridSearchCV(estimator = classifier,\n",
    "                               param_grid = tune_grid2,\n",
    "                                scoring = \"roc_auc\",\n",
    "                                n_jobs = nJobs,\n",
    "                                cv = tune_control,\n",
    "                                verbose = 5)\n",
    "\n",
    "    # Hyperparameter tuning and cross validation\n",
    "    tune_model2 = grid_search2.fit(X = X_train,\n",
    "                                 y = y_train,\n",
    "                                 **evaluation_parameters)\n",
    "    grid_search2.best_params_, grid_search2.best_score_\n",
    "    \n",
    "    # We can continue tunning the hyperparameters...\n",
    "    grid_search_last = grid_search2\n",
    "    \n",
    "    # Final model\n",
    "    print(\"=====Final Model=====\")\n",
    "    finalParameters = {'learning_rate': grid_search_last.best_params_.get('learning_rate'),\n",
    "                       'max_depth': grid_search_last.best_params_.get('max_depth'),\n",
    "                       'colsample_bytree': grid_search_last.best_params_.get('colsample_bytree'),\n",
    "                       'subsample': grid_search_last.best_params_.get('subsample'),\n",
    "                       'min_child_weight': grid_search_last.best_params_.get('min_child_weight'),\n",
    "                       'gamma': grid_search_last.best_params_.get('gamma'), \n",
    "                       'eval_metric': \"auc\",\n",
    "                       'objective': \"binary:logistic\"}\n",
    "    \n",
    "    finalModel = xgb.train(params = finalParameters,\n",
    "                   dtrain = Train,\n",
    "                   num_boost_round = 800,\n",
    "                   evals = [(Test, \"Yes\")],\n",
    "                   verbose_eval = 50)\n",
    "    \n",
    "    # Predictions on test dataset\n",
    "    predictions = finalModel.predict(Test)\n",
    "    predictions = np.where(predictions > 0.05, 1, 0)\n",
    "    \n",
    "    # Model Diagnostic\n",
    "    print(\"=====Model Diagnostic=====\")\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion Matrix')\n",
    "    fig.colorbar(cax)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print report\n",
    "    report = classification_report(y_test, predictions)\n",
    "    print(report)\n",
    "    \n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([-0.02, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # feature importances\n",
    "    xgb.plot_importance(finalModel, max_num_features = 10)\n",
    "    \n",
    "    # Prepare and plot SHAP \n",
    "    # monkey patch\n",
    "    model_bytearray = finalModel.save_raw()[4:]\n",
    "    def myfun(self=None):\n",
    "        return model_bytearray\n",
    "    finalModel.save_raw = myfun\n",
    "    # plot SHAP\n",
    "    explainer = shap.TreeExplainer(finalModel)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values,\n",
    "                      X_test,\n",
    "                      feature_names = feature_columns,\n",
    "                      max_display = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I am not sure if it is a time series problem. If it is a time series problem, we might split data (into training, validation and test) by the time point, and fine tune the hyperparameters differently. I used to fine tune the hyperparemeters using for loop, but I think there might be better ways..(?) \n",
    "\n",
    "2. for each features, shall we use \"lag\" to add new features like \"happy_lag_1\", \"happy_lag_2\", \"stress_lag_1\" etc. which will suggest the surveyee's mood at the last time(s) he took the survey. It might depend on the featrue importance plot(?) For example, if we know anxious is mostly related to is_craving, shall we add more lag of \"anxious\" in the features?\n",
    "\n",
    "3. Do we need to add some more features to for more accurate predictions? Maybe some domain knowledge are required.\n",
    "\n",
    "4. Further discussion on how to fill the NaNs and how to do the interpolation is needed. Maybe after the primary EDA on the real example. \n",
    "\n",
    "5. Is there any value we would like to return from the above function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
